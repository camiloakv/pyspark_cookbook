{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camiloakv/pyspark_cookbook/blob/main/pyspark_cookbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "chewrRjipjV8"
      },
      "source": [
        ">[Setup](#scrollTo=xPXzpKbYeBmx)\n",
        "\n",
        ">[Create DataFrame](#scrollTo=srFXZBCJpxvu)\n",
        "\n",
        ">>[Create from range](#scrollTo=gWPdNWv2Y0Q6)\n",
        "\n",
        ">>[Create from list](#scrollTo=BODWWdkHY0RC)\n",
        "\n",
        ">>[Create from list of tuples](#scrollTo=CH0erjoKY0RG)\n",
        "\n",
        ">>[Add index column](#scrollTo=tiy2MgkxY0RI)\n",
        "\n",
        ">>>[v0](#scrollTo=4yBO2gcnY0RJ)\n",
        "\n",
        ">>>[v1](#scrollTo=vDm7Rg1TY0RJ)\n",
        "\n",
        ">>[Add column with constant value](#scrollTo=ljeNbD39Y0RL)\n",
        "\n",
        ">[Extract from DataFrame](#scrollTo=S7H-8G5FsVfL)\n",
        "\n",
        ">>[Get first value of column](#scrollTo=eoj46n_eY0SI)\n",
        "\n",
        ">>[Get list from column](#scrollTo=hH3cuI1cY0Sc)\n",
        "\n",
        ">>[Get minimum/maximum from column](#scrollTo=jwtpz8qOY0Sd)\n",
        "\n",
        ">>>[v0: list minimum](#scrollTo=dQxUqEtFY0Sd)\n",
        "\n",
        ">>>[v1: agg](#scrollTo=HvAriCZ0Y0Se)\n",
        "\n",
        ">[Transformations](#scrollTo=Llr04sOjtJWL)\n",
        "\n",
        ">>[Shuffle rows](#scrollTo=7WC8rRMyY0Sb)\n",
        "\n",
        ">>[value_counts](#scrollTo=6eAIIqMDY0Sg)\n",
        "\n",
        ">>[Select n top rows per group](#scrollTo=wN4EfS8qY0Sh)\n",
        "\n",
        ">>[Pivot table](#scrollTo=0BdIEuGJY0RM)\n",
        "\n",
        ">>>[v0: use this!](#scrollTo=MgmtVcrBY0RM)\n",
        "\n",
        ">>>[v1: optimized (Spark $\\geq$ 2.0)](#scrollTo=kfvIt3YsY0RN)\n",
        "\n",
        ">>>[v2: optimized](#scrollTo=lZFQ6rdqY0RO)\n",
        "\n",
        ">>[Merge (concat)](#scrollTo=ZcH5tZ2EY0Se)\n",
        "\n",
        ">>>[v0: by column position](#scrollTo=Riil-1mbWJbY)\n",
        "\n",
        ">>>[v1: by column name (Spark >= 3.1)](#scrollTo=ZNAV8txkY0Sg)\n",
        "\n",
        ">>[Joins](#scrollTo=gulioPLWY0SO)\n",
        "\n",
        ">[Scratch / Work in progress (ignore)](#scrollTo=7EToxqmWaJPC)\n",
        "\n",
        ">>[Normalize list](#scrollTo=MvgUrLK9Y0SK)\n",
        "\n",
        ">>[Normalize column](#scrollTo=ZEuhAyeyY0SM)\n",
        "\n",
        ">>[Cosine similarity](#scrollTo=QkBafww5Y0RP)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPXzpKbYeBmx"
      },
      "source": [
        "# Setup\n",
        "\n",
        "https://medium.com/grabngoinfo/install-pyspark-3-on-google-colab-the-easy-way-577ec4a2bcd8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pwnujp4YZwF7",
        "outputId": "ed7fc007-5018-496d-faa6-95d565c43c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "p9n9QebgY0Qy",
        "outputId": "6e821f4d-2fbd-4542-813e-387267da6f8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f2561dbfaf0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://baed537e2db7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oXETvAS2Y0Q5"
      },
      "outputs": [],
      "source": [
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QsR4K3B9vhMm"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as sf\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "from pyspark.sql.window import Window\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srFXZBCJpxvu"
      },
      "source": [
        "# Create DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWPdNWv2Y0Q6"
      },
      "source": [
        "## Create from range\n",
        "\n",
        "https://stackoverflow.com/questions/36803030/pyspark-randomize-rows-in-dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OVKAA7QdY0RB"
      },
      "outputs": [],
      "source": [
        "df_from_range = sc.parallelize(range(10, 20)).map(lambda x: (x, )).toDF([\"x\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybkOrmzaia-w",
        "outputId": "80bf5941-49c2-4c55-c8ab-be8f6d4569cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|  x|\n",
            "+---+\n",
            "| 10|\n",
            "| 11|\n",
            "| 12|\n",
            "| 13|\n",
            "| 14|\n",
            "| 15|\n",
            "| 16|\n",
            "| 17|\n",
            "| 18|\n",
            "| 19|\n",
            "+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_from_range.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BODWWdkHY0RC"
      },
      "source": [
        "## Create from list\n",
        "\n",
        "https://stackoverflow.com/questions/36803030/pyspark-randomize-rows-in-dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D7nwcT8lY0RD"
      },
      "outputs": [],
      "source": [
        "mylist = [1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
        "df_from_list = spark.createDataFrame(mylist, IntegerType()).selectExpr(\"value as Fibonacci\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LImE-ck2Y0RD",
        "outputId": "ee3e640c-5571-4dda-a679-f0f26311fc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|Fibonacci|\n",
            "+---------+\n",
            "|        1|\n",
            "|        1|\n",
            "|        2|\n",
            "|        3|\n",
            "|        5|\n",
            "|        8|\n",
            "|       13|\n",
            "|       21|\n",
            "|       34|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_from_list.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yxLM5Ih2Y0RE"
      },
      "outputs": [],
      "source": [
        "df_from_list2 = spark.createDataFrame([1.234, 5.678], DoubleType()).selectExpr(\"value as float_nums\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0xV4l3XY0RF",
        "outputId": "eec25996-7f79-44f9-cecb-7c21f92107dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|float_nums|\n",
            "+----------+\n",
            "|     1.234|\n",
            "|     5.678|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_from_list2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH0erjoKY0RG"
      },
      "source": [
        "## Create from list of tuples\n",
        "\n",
        "https://sparkbyexamples.com/pyspark/pyspark-create-dataframe-from-list/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hXItvM5FY0RH"
      },
      "outputs": [],
      "source": [
        "lot = [\n",
        "    (1, 1),\n",
        "    (1, 2),\n",
        "    (2, 2),\n",
        "    (2, 1),\n",
        "    (2, 3),\n",
        "    (3, 5),\n",
        "    (3, 6),\n",
        "    (3, 3),\n",
        "    (3, 4),\n",
        "]\n",
        "lot_cols = [\"id\", \"item\"]\n",
        "df_lot = spark.createDataFrame(data=lot, schema=lot_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhVyoooyY0RI",
        "outputId": "c8284234-c890-4744-cd05-6d48ebc52cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|item|\n",
            "+---+----+\n",
            "|  1|   1|\n",
            "|  1|   2|\n",
            "|  2|   2|\n",
            "|  2|   1|\n",
            "|  2|   3|\n",
            "|  3|   5|\n",
            "|  3|   6|\n",
            "|  3|   3|\n",
            "|  3|   4|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_lot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiy2MgkxY0RI"
      },
      "source": [
        "## Add index column\n",
        "\n",
        "https://stackoverflow.com/questions/43406887/spark-dataframe-how-to-add-a-index-column-aka-distributed-data-index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yBO2gcnY0RJ"
      },
      "source": [
        "### v0\n",
        "\n",
        "Monotonically increasing, not necessarely consecutive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2unKaopsY0RJ"
      },
      "outputs": [],
      "source": [
        "df_index = df_from_range.select(\"*\").withColumn(\"index1\", sf.monotonically_increasing_id())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJRVuTp5Y0RJ",
        "outputId": "b7264a75-b4b1-4456-9233-ceba1bea3925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "|  x|    index1|\n",
            "+---+----------+\n",
            "| 10|         0|\n",
            "| 11|         1|\n",
            "| 12|         2|\n",
            "| 13|         3|\n",
            "| 14|         4|\n",
            "| 15|8589934592|\n",
            "| 16|8589934593|\n",
            "| 17|8589934594|\n",
            "| 18|8589934595|\n",
            "| 19|8589934596|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_index.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDm7Rg1TY0RJ"
      },
      "source": [
        "### v1\n",
        "Consecutive, starting from zero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0sQnV9JFY0RK"
      },
      "outputs": [],
      "source": [
        "df_index2 = df_from_range.withColumn(\n",
        "    \"index2\",\n",
        "    sf.row_number().over(Window.orderBy(sf.monotonically_increasing_id())) - 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlBW_em9Y0RK",
        "outputId": "1ab3d783-77db-45e7-aef9-73b956d4686b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "|  x|index2|\n",
            "+---+------+\n",
            "| 10|     0|\n",
            "| 11|     1|\n",
            "| 12|     2|\n",
            "| 13|     3|\n",
            "| 14|     4|\n",
            "| 15|     5|\n",
            "| 16|     6|\n",
            "| 17|     7|\n",
            "| 18|     8|\n",
            "| 19|     9|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_index2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljeNbD39Y0RL"
      },
      "source": [
        "## Add column with constant value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "E5oUOzUqY0RL"
      },
      "outputs": [],
      "source": [
        "df_const = df_index2.withColumn(\"val\", sf.lit(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pFZDvvtY0RL",
        "outputId": "6d479f34-868b-4ad6-cf69-64bc30b9da28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "|  x|index2|val|\n",
            "+---+------+---+\n",
            "| 10|     0|  1|\n",
            "| 11|     1|  1|\n",
            "| 12|     2|  1|\n",
            "| 13|     3|  1|\n",
            "| 14|     4|  1|\n",
            "| 15|     5|  1|\n",
            "| 16|     6|  1|\n",
            "| 17|     7|  1|\n",
            "| 18|     8|  1|\n",
            "| 19|     9|  1|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_const.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7H-8G5FsVfL"
      },
      "source": [
        "# Extract from DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoj46n_eY0SI"
      },
      "source": [
        "## Get first value of column\n",
        "\n",
        "https://stackoverflow.com/questions/56442215/how-to-get-first-value-and-last-value-from-dataframe-column-in-pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8ol8b9Y8Y0SJ"
      },
      "outputs": [],
      "source": [
        "first_record = df_const.collect()[0]['x']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaRhqVcHY0SK",
        "outputId": "651ae249-b2e8-47fc-dda0-8dacbf5961d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, int)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "first_record, type(first_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH3cuI1cY0Sc"
      },
      "source": [
        "## Get list from column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "h-PgzfwhY0Sc"
      },
      "outputs": [],
      "source": [
        "li = df_index.select(\"x\").rdd.flatMap(lambda x: x).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-Pg73CtY0Sd",
        "outputId": "6aebedf5-900c-4ebb-865c-6d6fdf878ea6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "li"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwtpz8qOY0Sd"
      },
      "source": [
        "## Get minimum/maximum from column\n",
        "https://stackoverflow.com/questions/33224740/best-way-to-get-the-max-value-in-a-spark-dataframe-column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQxUqEtFY0Sd"
      },
      "source": [
        "### v0: list minimum\n",
        "get list, then get minimum (see previous section)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvAriCZ0Y0Se"
      },
      "source": [
        "### v1: agg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnUPR2ShY0Se",
        "outputId": "5fc64660-6f4b-48f7-b479-1d75379d5f48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "df_index.agg({\"x\": \"min\"}).collect()[0][f\"min(x)\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llr04sOjtJWL"
      },
      "source": [
        "# Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WC8rRMyY0Sb"
      },
      "source": [
        "## Shuffle rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PMt21L-2Y0Sb"
      },
      "outputs": [],
      "source": [
        "df_shuffle = df_index.orderBy(sf.rand(42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiBU-RM5Y0Sb",
        "outputId": "5fe73574-2a51-4206-e7ef-aade8a3f4f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "|  x|    index1|\n",
            "+---+----------+\n",
            "| 18|8589934595|\n",
            "| 17|8589934594|\n",
            "| 13|         3|\n",
            "| 11|         1|\n",
            "| 10|         0|\n",
            "| 19|8589934596|\n",
            "| 16|8589934593|\n",
            "| 14|         4|\n",
            "| 15|8589934592|\n",
            "| 12|         2|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_shuffle.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eAIIqMDY0Sg"
      },
      "source": [
        "## value_counts\n",
        "\n",
        "== groupBy + count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk9RKkuA_gB1",
        "outputId": "b5d26969-accf-4e07-d5b9-d9635f023383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id|count|\n",
            "+---+-----+\n",
            "|  3|    4|\n",
            "|  2|    3|\n",
            "|  1|    2|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_lot.groupBy(\"id\").count().orderBy(sf.desc(\"count\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN4EfS8qY0Sh"
      },
      "source": [
        "## Select n top rows per group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Lmc7CUD4CT-6"
      },
      "outputs": [],
      "source": [
        "n_selected = 2\n",
        "w = Window.partitionBy(\"id\").orderBy(sf.col(\"item\"))\n",
        "df_selected = df_lot.withColumn(\"row\", sf.row_number().over(w))\\\n",
        "  .filter(sf.col(\"row\") <= n_selected)\\\n",
        "  .drop(sf.col(\"row\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHRsPz3dLKBQ",
        "outputId": "6594cd08-0daa-47e5-f8d1-d7cdfe5d0ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|item|\n",
            "+---+----+\n",
            "|  1|   1|\n",
            "|  1|   2|\n",
            "|  2|   1|\n",
            "|  2|   2|\n",
            "|  3|   3|\n",
            "|  3|   4|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_selected.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BdIEuGJY0RM"
      },
      "source": [
        "## Pivot table\n",
        "\n",
        "https://sparkbyexamples.com/pyspark/pyspark-pivot-and-unpivot-dataframe/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgmtVcrBY0RM"
      },
      "source": [
        "### v0: use this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xTScFZPwY0RM"
      },
      "outputs": [],
      "source": [
        "df_pivot = df_const.groupBy(\"index2\").pivot(\"x\").sum(\"val\").fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLZnWqSpmj6N",
        "outputId": "2463cfc1-1e92-48c6-f7b4-f790ec5e9c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+---+---+---+---+---+---+---+---+---+\n",
            "|index2| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19|\n",
            "+------+---+---+---+---+---+---+---+---+---+---+\n",
            "|     0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
            "|     1|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|\n",
            "|     2|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|\n",
            "|     3|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
            "|     4|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
            "|     5|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|\n",
            "|     6|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
            "|     7|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|\n",
            "|     8|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|\n",
            "|     9|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
            "+------+---+---+---+---+---+---+---+---+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pivot.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aXYN-ZVgY0RN"
      },
      "outputs": [],
      "source": [
        "# TESTING PERFORMANCE\n",
        "#df_index3 = spark.createDataFrame(range(100), IntegerType()).selectExpr(\"value as x\")\n",
        "#df_index3 = df_index3.withColumn('index', sf.row_number().over(Window.orderBy(sf.monotonically_increasing_id())) - 1)\n",
        "#df_const2 = df_index3.withColumn(\"val\", sf.lit(1))\n",
        "#df_pivot = df_const2.groupBy(\"index\").pivot(\"x\").sum(\"val\").fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6BEzYW2YY0RN"
      },
      "outputs": [],
      "source": [
        "#df_pivot.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfvIt3YsY0RN"
      },
      "source": [
        "### v1: optimized (Spark $\\geq$ 2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZlgaJhsBY0RN"
      },
      "outputs": [],
      "source": [
        "df_pivot1 = df_const.groupBy(\"index2\").pivot(\"x\", range(10, 20)).sum(\"val\").fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XX7PVDCY0RO",
        "outputId": "65b8cb12-8b8d-48d7-8f4d-ce3f00867438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+---+---+---+---+---+---+---+---+---+\n",
            "|index2| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19|\n",
            "+------+---+---+---+---+---+---+---+---+---+---+\n",
            "|     0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
            "|     1|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|\n",
            "|     2|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|\n",
            "|     3|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
            "|     4|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
            "|     5|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|\n",
            "|     6|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
            "|     7|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|\n",
            "|     8|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|\n",
            "|     9|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
            "+------+---+---+---+---+---+---+---+---+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pivot1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZFQ6rdqY0RO"
      },
      "source": [
        "### v2: optimized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fHeXwb2pY0RO"
      },
      "outputs": [],
      "source": [
        "df_pivot2 = df_const \\\n",
        "    .groupBy(\"index2\", \"x\") \\\n",
        "    .sum(\"val\") \\\n",
        "    .groupBy(\"index2\") \\\n",
        "    .pivot(\"x\") \\\n",
        "    .sum(\"sum(val)\") \\\n",
        "    .fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ8OF_-LY0RO",
        "outputId": "66599c48-c0e9-4b48-b89f-52ce07b0d401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+---+---+---+---+---+---+---+---+---+\n",
            "|index2| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19|\n",
            "+------+---+---+---+---+---+---+---+---+---+---+\n",
            "|     0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
            "|     1|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|\n",
            "|     2|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|\n",
            "|     3|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
            "|     4|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
            "|     5|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|\n",
            "|     6|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
            "|     7|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|\n",
            "|     8|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|\n",
            "|     9|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
            "+------+---+---+---+---+---+---+---+---+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pivot2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD5bVbvCY0RO"
      },
      "source": [
        "__Note:__ v0 outperforms v2, use that one!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcH5tZ2EY0Se"
      },
      "source": [
        "## Merge (concat)\n",
        "https://walkenho.github.io/merging-multiple-dataframes-in-pyspark/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Riil-1mbWJbY"
      },
      "source": [
        "### v0: by column position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "m96RrfusY0Sf"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "from pyspark.sql import DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "b6MXgKTmY0Sf"
      },
      "outputs": [],
      "source": [
        "df_index_concat = reduce(DataFrame.unionAll, [df_index, df_index2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbI7CzgcY0Sf",
        "outputId": "adbcec5e-d945-451b-dabd-eae6ddd1b81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "|  x|    index1|\n",
            "+---+----------+\n",
            "| 10|         0|\n",
            "| 11|         1|\n",
            "| 12|         2|\n",
            "| 13|         3|\n",
            "| 14|         4|\n",
            "| 15|8589934592|\n",
            "| 16|8589934593|\n",
            "| 17|8589934594|\n",
            "| 18|8589934595|\n",
            "| 19|8589934596|\n",
            "| 10|         0|\n",
            "| 11|         1|\n",
            "| 12|         2|\n",
            "| 13|         3|\n",
            "| 14|         4|\n",
            "| 15|         5|\n",
            "| 16|         6|\n",
            "| 17|         7|\n",
            "| 18|         8|\n",
            "| 19|         9|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_index_concat.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNAV8txkY0Sg"
      },
      "source": [
        "### v1: by column name (Spark >= 3.1)\n",
        "https://sparkbyexamples.com/spark/spark-merge-two-dataframes-with-different-columns/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WhgXZ2DG9ZLI"
      },
      "outputs": [],
      "source": [
        "df_merge = df_index.unionByName(df_index2, allowMissingColumns=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY1rNyL_9eUP",
        "outputId": "8930cea1-1633-42fd-9ae5-553bcc0a5a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+\n",
            "|  x|    index1|index2|\n",
            "+---+----------+------+\n",
            "| 10|         0|  null|\n",
            "| 11|         1|  null|\n",
            "| 12|         2|  null|\n",
            "| 13|         3|  null|\n",
            "| 14|         4|  null|\n",
            "| 15|8589934592|  null|\n",
            "| 16|8589934593|  null|\n",
            "| 17|8589934594|  null|\n",
            "| 18|8589934595|  null|\n",
            "| 19|8589934596|  null|\n",
            "| 10|      null|     0|\n",
            "| 11|      null|     1|\n",
            "| 12|      null|     2|\n",
            "| 13|      null|     3|\n",
            "| 14|      null|     4|\n",
            "| 15|      null|     5|\n",
            "| 16|      null|     6|\n",
            "| 17|      null|     7|\n",
            "| 18|      null|     8|\n",
            "| 19|      null|     9|\n",
            "+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_merge.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gulioPLWY0SO"
      },
      "source": [
        "## Joins\n",
        "\n",
        "https://www.learnbymarketing.com/1100/pyspark-joins-by-example/\n",
        "\n",
        "https://stackoverflow.com/questions/46944493/removing-duplicate-columns-after-a-df-join-in-spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TqXBMFD-fc3i"
      },
      "outputs": [],
      "source": [
        "dfl = df_index.filter(sf.col(\"x\") < 18)\n",
        "dfr = df_index2.filter(sf.col(\"x\") > 11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlC23CIAh3mh",
        "outputId": "ecab9aa5-7f35-428e-f72c-8b0c1b278ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+\n",
            "|  x|    index1|index2|\n",
            "+---+----------+------+\n",
            "| 10|         0|  null|\n",
            "| 11|         1|  null|\n",
            "| 12|         2|     2|\n",
            "| 13|         3|     3|\n",
            "| 14|         4|     4|\n",
            "| 15|8589934592|     5|\n",
            "| 16|8589934593|     6|\n",
            "| 17|8589934594|     7|\n",
            "+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfl.join(dfr, [\"x\"], how='left').orderBy(sf.asc(\"x\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3b3KJqvgV-i",
        "outputId": "0331fbe5-a0df-4388-e4a3-f93eeafcabda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+\n",
            "|  x|    index1|index2|\n",
            "+---+----------+------+\n",
            "| 12|         2|     2|\n",
            "| 13|         3|     3|\n",
            "| 14|         4|     4|\n",
            "| 15|8589934592|     5|\n",
            "| 16|8589934593|     6|\n",
            "| 17|8589934594|     7|\n",
            "| 18|      null|     8|\n",
            "| 19|      null|     9|\n",
            "+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfl.join(dfr, [\"x\"], how='right').orderBy(sf.asc(\"x\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfl.join(dfr, [\"x\"], how='inner').orderBy(sf.asc(\"x\")).show()  # default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQkMHRcSFDU2",
        "outputId": "56a7eced-8b4f-4389-e09b-565930e5d917"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+\n",
            "|  x|    index1|index2|\n",
            "+---+----------+------+\n",
            "| 12|         2|     2|\n",
            "| 13|         3|     3|\n",
            "| 14|         4|     4|\n",
            "| 15|8589934592|     5|\n",
            "| 16|8589934593|     6|\n",
            "| 17|8589934594|     7|\n",
            "+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfl.join(dfr, [\"x\"], how='outer').orderBy(sf.asc(\"x\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmuz7_UMFFmz",
        "outputId": "b112c5c4-88ef-4edd-f299-6f5fba11d0c0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+\n",
            "|  x|    index1|index2|\n",
            "+---+----------+------+\n",
            "| 10|         0|  null|\n",
            "| 11|         1|  null|\n",
            "| 12|         2|     2|\n",
            "| 13|         3|     3|\n",
            "| 14|         4|     4|\n",
            "| 15|8589934592|     5|\n",
            "| 16|8589934593|     6|\n",
            "| 17|8589934594|     7|\n",
            "| 18|      null|     8|\n",
            "| 19|      null|     9|\n",
            "+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can always use full outer join and filter the rows required:"
      ],
      "metadata": {
        "id": "rz8SPIthQkax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# equivalent to left join\n",
        "dfl\\\n",
        "  .join(dfr, [\"x\"], how='full_outer')\\\n",
        "  .orderBy(sf.asc(\"x\"))\\\n",
        "  .filter(~sf.col(\"index1\").isNull())\\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-VE3NIfPLzV",
        "outputId": "305c5138-7b20-4410-fd43-74b3c98bccca"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+\n",
            "|  x|    index1|index2|\n",
            "+---+----------+------+\n",
            "| 10|         0|  null|\n",
            "| 11|         1|  null|\n",
            "| 12|         2|     2|\n",
            "| 13|         3|     3|\n",
            "| 14|         4|     4|\n",
            "| 15|8589934592|     5|\n",
            "| 16|8589934593|     6|\n",
            "| 17|8589934594|     7|\n",
            "+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "y6ka0T_8Y0Sg"
      },
      "outputs": [],
      "source": [
        "#spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FofITL1aQ6w"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EToxqmWaJPC"
      },
      "source": [
        "# Scratch / Work in progress (ignore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvgUrLK9Y0SK"
      },
      "source": [
        "## Normalize list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-SvCvQyY0SK"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.feature import Normalizer\n",
        "\n",
        "nor = Normalizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6j6XiPqY0SL",
        "outputId": "9c64ea39-d692-433a-b750-54b2acb172b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DenseVector([0.1826, 0.3651, 0.5477, 0.7303])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nor.transform([1,2,3,4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEuhAyeyY0SM"
      },
      "source": [
        "## Normalize column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QIBY8AAY0SM"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Normalizer\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"f2_norm\")\n",
        "df_pivot_feats_norm = normalizer.transform(df_pivot_feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT_2eGwdY0SN"
      },
      "outputs": [],
      "source": [
        "x = df_pivot_feats.collect()[0]['features']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7-wMAnPY0SN",
        "outputId": "ecfbedf2-ef52-4ce2-c647-1ebc651ea467"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyspark.ml.linalg.SparseVector"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBewaJbiY0SO"
      },
      "source": [
        "__Note:__ Needs `pyspark.ml.feature` and `SparseVector`-like data type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkBafww5Y0RP"
      },
      "source": [
        "## Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS5ZjR5EY0RP"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[x for x in df_pivot.columns if x not in [\"index\"]],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "df_pivot_feats = assembler.transform(df_pivot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaEc7SIPY0R-",
        "outputId": "0d3e3bea-1ea7-49e6-8126-5d75f7d80dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---+---+---+---+---+---+---+---+---+---+--------------+\n",
            "|index| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19|      features|\n",
            "+-----+---+---+---+---+---+---+---+---+---+---+--------------+\n",
            "|    0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|(10,[0],[1.0])|\n",
            "|    1|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|(10,[1],[1.0])|\n",
            "|    2|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|(10,[2],[1.0])|\n",
            "|    3|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|(10,[3],[1.0])|\n",
            "|    4|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|(10,[4],[1.0])|\n",
            "|    5|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|(10,[5],[1.0])|\n",
            "|    6|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|(10,[6],[1.0])|\n",
            "|    7|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|(10,[7],[1.0])|\n",
            "|    8|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|(10,[8],[1.0])|\n",
            "|    9|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|(10,[9],[1.0])|\n",
            "+-----+---+---+---+---+---+---+---+---+---+---+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pivot_feats.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS8lVB41Y0R-"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import SparseVector, DenseVector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AayeQmSOY0R_"
      },
      "outputs": [],
      "source": [
        "##df_pivot_feats.withColumn(\"f2\", DenseVector(sf.col(\"features\").toArray())).show()\n",
        "#df_pivot_feats.withColumn(\"f2\", DenseVector(sf.col(\"features\"))).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H78UATKY0SA"
      },
      "source": [
        "Convert to dense\n",
        "\n",
        "https://stackoverflow.com/questions/58490770/convert-pyspark-densevector-to-array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh-QbLl5Y0SA"
      },
      "outputs": [],
      "source": [
        "## v0\n",
        "#import pyspark.sql.types as st\n",
        "#\n",
        "#to_array = sf.udf(lambda v: v.toArray().tolist(), st.ArrayType(st.FloatType()))\n",
        "#df_pivot_feats = df_pivot_feats.withColumn('f2', to_array('features'))\n",
        "\n",
        "# v1 (spark >= 3.0)\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "df_pivot_feats = df_pivot_feats.withColumn('f2', vector_to_array('features'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWmgoZ8SY0SB"
      },
      "source": [
        "https://stackoverflow.com/questions/46758768/calculating-the-cosine-similarity-between-all-the-rows-of-a-dataframe-in-pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV9wJks4Y0SC"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
        "mat = IndexedRowMatrix(df_pivot_feats.select(\"index\", \"f2\").rdd).toBlockMatrix()\n",
        "dot = mat.multiply(mat.transpose())\n",
        "#similarity_matrix = dot.toLocalMatrix().toArray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swBVSnHuY0SD",
        "outputId": "2d5c0b8a-2fe7-4393-8341-1797c16c7623"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 10)"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dot.numCols(), dot.numRows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cykwdQGgY0SE"
      },
      "outputs": [],
      "source": [
        "def vvec(nrows, def_val=1):\n",
        "\n",
        "    df_vvec = spark.createDataFrame(range(nrows), IntegerType()).selectExpr(\"value as index\")\n",
        "\n",
        "    df_vvec = df_vvec.withColumn(\"val\", sf.lit(def_val))\n",
        "\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[x for x in df_vvec.columns if x not in [\"index\"]],\n",
        "        outputCol=\"features\"\n",
        "    )\n",
        "\n",
        "    df_vvec = assembler.transform(df_vvec)\n",
        "\n",
        "    from pyspark.ml.functions import vector_to_array\n",
        "    df_vvec = df_vvec.withColumn('f2', vector_to_array('features'))\n",
        "\n",
        "    vvec = IndexedRowMatrix(df_vvec.select(\"index\", \"f2\").rdd).toBlockMatrix()\n",
        "\n",
        "    return vvec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY97lnRYY0SF"
      },
      "outputs": [],
      "source": [
        "vv = vvec(dot.numCols())\n",
        "soma_sims = dot.multiply(vv)\n",
        "soma_sims = vv.transpose().multiply(soma_sims)\n",
        "soma_sims = soma_sims.toLocalMatrix().toArray()\n",
        "soma_sims = soma_sims[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlCbBwk3Y0SF",
        "outputId": "7a14d863-13c9-4319-e1a7-f9acb4257eb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10.0"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "soma_sims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsLxB9J7Y0SG",
        "outputId": "18347b8e-9f0a-46d6-ec35-21b621ca6ed5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mat.numCols()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khw0y21CY0SH",
        "outputId": "3c9bcac2-4025-4abf-8166-e86fcf0ba872"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mat.numRows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWslh63vY0SI"
      },
      "source": [
        "Alternatively, to get the mean similarity:\n",
        "\n",
        "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.linalg.distributed.IndexedRowMatrix.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWVI04LMY0SP"
      },
      "outputs": [],
      "source": [
        "#spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_xCGn-qY0SP"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3mM8SB0Y0SP"
      },
      "outputs": [],
      "source": [
        "mat2 = IndexedRowMatrix(df_pivot_feats.select(\"index\", \"f2\").rdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCb55CnfY0SQ"
      },
      "outputs": [],
      "source": [
        "cs = mat2.columnSimilarities()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quYElszxY0SQ"
      },
      "outputs": [],
      "source": [
        "dir(cs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o13_I_cY0SR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lpw4As3Y0SR"
      },
      "outputs": [],
      "source": [
        "df_pivot_feats = df_pivot_feats.withColumn(\"index\", sf.col(\"index\").cast('bigint'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_Goh-zYY0SR"
      },
      "outputs": [],
      "source": [
        "r2 = df_pivot_feats.select(\"index\", \"features\").rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBKt96aVY0SS"
      },
      "outputs": [],
      "source": [
        "type(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mXb9vFIY0SS"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
        "mat = IndexedRowMatrix(df_pivot_feats.select(\"index\", \"f2\").rdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56DHslLKY0SS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_bld0_7Y0SS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ6J5BgsY0ST"
      },
      "outputs": [],
      "source": [
        "dfx = df_pivot_feats.select(\"index\", \"features\").rdd.map(tuple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsVchpb_Y0ST",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "type(dfx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2mWZJS2Y0ST"
      },
      "outputs": [],
      "source": [
        "IndexedRowMatrix?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiQ2xqP6Y0ST"
      },
      "outputs": [],
      "source": [
        "df_pivot_feats.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEkTLIwsY0SU"
      },
      "outputs": [],
      "source": [
        "r2 = df_pivot_feats.select(\"index\", \"features\").rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcNmHxrXY0SU"
      },
      "outputs": [],
      "source": [
        "type(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67g5QWwQY0SU"
      },
      "outputs": [],
      "source": [
        "mat = IndexedRowMatrix(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BInagBuGY0SV"
      },
      "source": [
        "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.linalg.distributed.IndexedRowMatrix.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Zkv2M6QY0SV"
      },
      "outputs": [],
      "source": [
        "rows = sc.parallelize([IndexedRow(0, [1, 2, 3]),\n",
        "                       IndexedRow(6, [4, 5, 6])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBt8C3HCY0SV"
      },
      "outputs": [],
      "source": [
        "type(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QX-xUspY0SV"
      },
      "outputs": [],
      "source": [
        "rows.toDF().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmF20hdOY0SW"
      },
      "outputs": [],
      "source": [
        "df_pivot_feats.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J7iF1ZdY0SW"
      },
      "outputs": [],
      "source": [
        "mat = IndexedRowMatrix(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_kwIkMaY0SW"
      },
      "outputs": [],
      "source": [
        "cs = mat.columnSimilarities()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymg9o40LY0SW"
      },
      "outputs": [],
      "source": [
        "type(cs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEXkStFgY0SX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLeRs_TdY0SX"
      },
      "outputs": [],
      "source": [
        "np.ndarray(cs.toBlockMatrix())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCjm-hFFY0SX"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCofX11GY0SY"
      },
      "outputs": [],
      "source": [
        "rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS5Om11kY0SY"
      },
      "outputs": [],
      "source": [
        "print(rows.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCDCukpWY0SY"
      },
      "outputs": [],
      "source": [
        "r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgag7g6KY0SY"
      },
      "outputs": [],
      "source": [
        "print(r2.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK6fFE6ZY0SZ"
      },
      "outputs": [],
      "source": [
        "mat = IndexedRowMatrix(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jAs-n-vY0SZ"
      },
      "outputs": [],
      "source": [
        "data = df_pivot_feats.select(\"index\", \"features\")\n",
        "\n",
        "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
        "mat = IndexedRowMatrix(data).toBlockMatrix()\n",
        "dot = mat.multiply(mat.transpose())\n",
        "dot.toLocalMatrix().toArray()\n",
        "\n",
        "#from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
        "#mat = IndexedRowMatrix(\n",
        "#    data.select(\"index\", \"features\")\\\n",
        "#        .rdd.map(lambda row: IndexedRow(row.index, row.features))).toBlockMatrix()\n",
        "##        .rdd.map(lambda row: IndexedRow(row.index, row.features.toArray()))).toBlockMatrix()\n",
        "#dot = mat.multiply(mat.transpose())\n",
        "#dot.toLocalMatrix().toArray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHPCyNwGY0SZ"
      },
      "source": [
        "(Pearson correlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bL9XiVcY0SZ"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
        "from pyspark.ml.stat import Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEuBWxI_Y0Sa"
      },
      "outputs": [],
      "source": [
        "pearsonCorr = Correlation.corr(df_pivot_feats, 'features', 'spearman').collect()[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS54zk-OY0Sa"
      },
      "outputs": [],
      "source": [
        "type(pearsonCorr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIoLbv6zY0Sa"
      },
      "outputs": [],
      "source": [
        "pearsonCorr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ntGJ6G-Y0Sb"
      },
      "outputs": [],
      "source": [
        "print(str(pearsonCorr).replace('nan', 'NaN'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60O0IRkaY0Sh"
      },
      "source": [
        "MISC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_4stkvkY0Sh"
      },
      "source": [
        "`df_temp.selectExpr('percentile(P2, 0.90)').show()`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}